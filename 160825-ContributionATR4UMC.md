---
title: 读文笔记-Contribution to ATR system for Underwater Mine Classification
tags: 目标识别,读文笔记,水下
grammar_cjkRuby: true
---

## 摘要
本文处理水下目标自动识别的几个问题并将其运用在水雷分类问题上。本文的贡献主要集中在**特征选择**与**目标分类**上。对于特征选择，文章提出基于一个成熟的filter方法基础上利用一个新颖的度量方法，CRM（composite relevance measure），特征的相关测量 ，如互信息和relief weight，只能评估特征的特定方面，CRM结合了几种度量方式以便于其提供更为综合的特征度量。无论线性还是非线性的度量方式均被考虑到。wide范围的分类器能够提供满意的分类结果使用CRM后。第二个，对于目标分类，提出基于DS理论的集成学习方案，基于DS理念融合不同学习器的分类结果，作者提出一个可理解的结构Basic Belief Assignment（BBA）。BBA既考虑了分类器的可靠性，又考虑了单一分类器对假设的支持度。

## 1 概述
ATR在水下应用中是过去的近二十年在大问题。由于现在SAS能够提供高质量图片，其典型的工作过程如下图所示。
![](http://ww1.sinaimg.cn/large/4542e63ejw1f76mqog8l8j20na06pdhi.jpg)

>(1) 什么是特征选择
特征选择 ( Feature Selection )也称特征子集选择( Feature Subset Selection , FSS ) ，或属性选择( Attribute Selection ) ，是指从全部特征中选取一个特征子集，使构造出来的模型更好。
(2) 为什么要做特征选择
       在机器学习的实际应用中，特征数量往往较多，其中可能存在不相关的特征，特征之间也可能存在相互依赖，容易导致如下的后果： 
       + 特征个数越多，分析特征、训练模型所需的时间就越长。
       +  特征个数越多，容易引起“维度灾难”，模型也会越复杂，其推广能力会下降。
(3) 特征选择的过程
特征选择过程一般包括产生过程，评价函数，停止准则，验证过程，这4个部分。
>
![特征选择的过程 ( M. Dash and H. Liu 1997 )](http://ww1.sinaimg.cn/large/4542e63ejw1f76m4d6yhmj20i90733yo.jpg)
>　
(1) 产生过程( Generation Procedure )产生过程是搜索特征子集的过程，负责为评价函数提供特征子集。搜索特征子集的过程有多种。
(2) 评价函数( Evaluation Function) 评价函数是评价一个特征子集好坏程度的一个准则。
(3) 停止准则( Stopping Criterion ) 停止准则是与评价函数相关的，一般是一个阈值，当评价函数值达到这个阈值后就可停止搜索。
(4) 验证过程( Validation Procedure )在验证数据集上验证选出来的特征子集的有效性。

>（1）产生过程是搜索特征子空间的过程。搜索的算法分为完全搜索(Complete)，启发式搜索(Heuristic)，随机搜索(Random) 3大类。
>![](http://ww4.sinaimg.cn/large/4542e63ejw1f76m899wokj20ia08laaf.jpg)
>（2）评价函数的作用是评价产生过程所提供的特征子集的好坏。 评价函数根据其工作原理，主要分为筛选器(Filter)、封装器( Wrapper )两大类。
>筛选器通过分析特征子集内部的特点来衡量其好坏。筛选器一般用作预处理，与分类器的选择无关。
>![](http://pic002.cnblogs.com/images/2011/63234/2011090620205673.jpg)
>封装器实质上是一个分类器，封装器用选取的特征子集对样本集进行分类，分类的精度作为衡量特征子集好坏的标准。
>![](http://pic002.cnblogs.com/images/2011/63234/2011090620213253.jpg)
>常见的评价函数。
　　(1) 相关性( Correlation)
       　　运用相关性来度量特征子集的好坏是基于这样一个假设：好的特征子集所包含的特征应该是与分类的相关度较高（相关度高），而特征之间相关度较低的（亢余度低）。可以使用线性相关系数(correlation coefficient) 来衡量向量之间线性相关度。
![](http://pic002.cnblogs.com/images/2011/63234/2011090620220822.png)
　　( 2) 距离 (Distance Metrics )
       　　运用距离度量进行特征选择是基于这样的假设：好的特征子集应该使得属于同一类的样本距离尽可能小，属于不同类的样本之间的距离尽可能远。
       　　常用的距离度量（相似性度量）包括欧氏距离、标准化欧氏距离、马氏距离等。
　　(3) 信息增益( Information Gain )假设存在离散变量Y，Y中的取值包括{y1，y2，....，ym} ，yi出现的概率为Pi。则Y的信息熵定义为：
![](http://pic002.cnblogs.com/images/2011/63234/2011090620241669.png)
　　　　信息熵有如下特性：若集合Y的元素分布越“纯”，则其信息熵越小；若Y分布越“紊乱”，则其信息熵越大。在极端的情况下：若Y只能取一个值，即P1=1，则H(Y)取最小值0；反之若各种取值出现的概率都相等，即都是1/m，则H(Y)取最大值log2m。
       　　在附加条件另一个变量X，而且知道X=xi后，Y的条件信息熵(Conditional Entropy)表示为：
				![](http://pic002.cnblogs.com/images/2011/63234/2011090620245858.png)
　　在加入条件X前后的Y的信息增益定义为
		![](http://pic002.cnblogs.com/images/2011/63234/2011090620253263.png)
　　　　类似的，分类标记C的信息熵H( C )可表示为：
			![](http://pic002.cnblogs.com/images/2011/63234/2011090620261033.png)
　　将特征Fj用于分类后的分类C的条件信息熵H( C | Fj )表示为：
	![](http://pic002.cnblogs.com/images/2011/63234/2011090620265132.png)
　　　　选用特征Fj前后的C的信息熵的变化成为C的信息增益(Information Gain)，用IG表示，公式为：
    	![](http://pic002.cnblogs.com/images/2011/63234/2011090620273361.png)
　　假设存在特征子集A和特征子集B，分类变量为C，若IG( C|A ) > IG( C|B ) ，则认为选用特征子集A的分类结果比B好，因此倾向于选用特征子集A。
  (4)一致性( Consistency )
       　　　　若样本1与样本2属于不同的分类，但在特征A、 B上的取值完全一样，那么特征子集{A，B}不应该选作最终的特征集。    
(5)分类器错误率 (Classifier error rate )
       　　使用特定的分类器，用给定的特征子集对样本集进行分类，用分类的精度来衡量特征子集的好坏。
　　PS:以上5种度量方法中，相关性、距离、信息增益、一致性属于筛选器，而分类器错误率属于封装器。

 >筛选器由于与具体的分类算法无关，因此其在不同的分类算法之间的推广能力较强，而且计算量也较小。而封装器由于在评价的过程中应用了具体的分类算法进行分类，因此其推广到其他分类算法的效果可能较差，而且计算量也较大。


 

特征选择能剔除不相关(irrelevant)或亢余(redundant )的特征，从而达到减少特征个数，提高模型精确度，减少运行时间的目的。另一方面，选取出真正相关的特征简化了模型，使研究人员易于理解数据产生的过程。



