---
title: 读文笔记-Contribution to ATR system for Underwater Mine Classification
tags: 目标识别,读文笔记,水下
grammar_cjkRuby: true
---

## 摘要
本文处理水下目标自动识别的几个问题并将其运用在水雷分类问题上。本文的贡献主要集中在**特征选择**与**目标分类**上。对于特征选择，文章提出基于一个成熟的filter方法基础上利用一个新颖的度量方法，CRM（composite relevance measure），特征的相关测量 ，如互信息和relief weight，只能评估特征的特定方面，CRM结合了几种度量方式以便于其提供更为综合的特征度量。无论线性还是非线性的度量方式均被考虑到。wide范围的分类器能够提供满意的分类结果使用CRM后。第二个，对于目标分类，提出基于DS理论的集成学习方案，基于DS理念融合不同学习器的分类结果，作者提出一个可理解的结构Basic Belief Assignment（BBA）。BBA既考虑了分类器的可靠性，又考虑了单一分类器对假设的支持度。

## 1 概述
ATR在水下应用中是过去的近二十年在大问题。由于现在SAS能够提供高质量图片，其典型的工作过程如下图所示。
![](http://ww1.sinaimg.cn/large/4542e63ejw1f76mqog8l8j20na06pdhi.jpg)

>(1) 什么是特征选择
特征选择 ( Feature Selection )也称特征子集选择( Feature Subset Selection , FSS ) ，或属性选择( Attribute Selection ) ，是指从全部特征中选取一个特征子集，使构造出来的模型更好。
(2) 为什么要做特征选择
       在机器学习的实际应用中，特征数量往往较多，其中可能存在不相关的特征，特征之间也可能存在相互依赖，容易导致如下的后果： 
       + 特征个数越多，分析特征、训练模型所需的时间就越长。
       +  特征个数越多，容易引起“维度灾难”，模型也会越复杂，其推广能力会下降。
(3) 特征选择的过程
特征选择过程一般包括产生过程，评价函数，停止准则，验证过程，这4个部分。
>
![特征选择的过程 ( M. Dash and H. Liu 1997 )](http://ww1.sinaimg.cn/large/4542e63ejw1f76m4d6yhmj20i90733yo.jpg)
>　
(1) 产生过程( Generation Procedure )产生过程是搜索特征子集的过程，负责为评价函数提供特征子集。搜索特征子集的过程有多种。
(2) 评价函数( Evaluation Function) 评价函数是评价一个特征子集好坏程度的一个准则。
(3) 停止准则( Stopping Criterion ) 停止准则是与评价函数相关的，一般是一个阈值，当评价函数值达到这个阈值后就可停止搜索。
(4) 验证过程( Validation Procedure )在验证数据集上验证选出来的特征子集的有效性。

>（1）产生过程是搜索特征子空间的过程。搜索的算法分为完全搜索(Complete)，启发式搜索(Heuristic)，随机搜索(Random) 3大类。
>![](http://ww4.sinaimg.cn/large/4542e63ejw1f76m899wokj20ia08laaf.jpg)
>（2）评价函数的作用是评价产生过程所提供的特征子集的好坏。 评价函数根据其工作原理，主要分为筛选器(Filter)、封装器( Wrapper )两大类。
>筛选器通过分析特征子集内部的特点来衡量其好坏。筛选器一般用作预处理，与分类器的选择无关。
>![](http://pic002.cnblogs.com/images/2011/63234/2011090620205673.jpg)
>
>封装器实质上是一个分类器，封装器用选取的特征子集对样本集进行分类，分类的精度作为衡量特征子集好坏的标准。
>
>![](http://pic002.cnblogs.com/images/2011/63234/2011090620213253.jpg)
>
>常见的评价函数。
　　(1) 相关性( Correlation)
       　　运用相关性来度量特征子集的好坏是基于这样一个假设：好的特征子集所包含的特征应该是与分类的相关度较高（相关度高），而特征之间相关度较低的（亢余度低）。可以使用线性相关系数(correlation coefficient) 来衡量向量之间线性相关度。
![](http://pic002.cnblogs.com/images/2011/63234/2011090620220822.png)
　　( 2) 距离 (Distance Metrics )
       　　运用距离度量进行特征选择是基于这样的假设：好的特征子集应该使得属于同一类的样本距离尽可能小，属于不同类的样本之间的距离尽可能远。
       　　常用的距离度量（相似性度量）包括欧氏距离、标准化欧氏距离、马氏距离等。
　　(3) 信息增益( Information Gain )假设存在离散变量Y，Y中的取值包括{y1，y2，....，ym} ，yi出现的概率为Pi。则Y的信息熵定义为：
![](http://pic002.cnblogs.com/images/2011/63234/2011090620241669.png)
　　　　信息熵有如下特性：若集合Y的元素分布越“纯”，则其信息熵越小；若Y分布越“紊乱”，则其信息熵越大。在极端的情况下：若Y只能取一个值，即P1=1，则H(Y)取最小值0；反之若各种取值出现的概率都相等，即都是1/m，则H(Y)取最大值log2m。
       　　在附加条件另一个变量X，而且知道X=xi后，Y的条件信息熵(Conditional Entropy)表示为：
				![](http://pic002.cnblogs.com/images/2011/63234/2011090620245858.png)
　　在加入条件X前后的Y的信息增益定义为
		![](http://pic002.cnblogs.com/images/2011/63234/2011090620253263.png)
　　　　类似的，分类标记C的信息熵H( C )可表示为：
			![](http://pic002.cnblogs.com/images/2011/63234/2011090620261033.png)
　　将特征Fj用于分类后的分类C的条件信息熵H( C | Fj )表示为：
	![](http://pic002.cnblogs.com/images/2011/63234/2011090620265132.png)
　　　　选用特征Fj前后的C的信息熵的变化成为C的信息增益(Information Gain)，用IG表示，公式为：
    	![](http://pic002.cnblogs.com/images/2011/63234/2011090620273361.png)
　　假设存在特征子集A和特征子集B，分类变量为C，若IG( C|A ) > IG( C|B ) ，则认为选用特征子集A的分类结果比B好，因此倾向于选用特征子集A。
  (4)一致性( Consistency )
       　　　　若样本1与样本2属于不同的分类，但在特征A、 B上的取值完全一样，那么特征子集{A，B}不应该选作最终的特征集。    
(5)分类器错误率 (Classifier error rate )
       　　使用特定的分类器，用给定的特征子集对样本集进行分类，用分类的精度来衡量特征子集的好坏。
　　PS:以上5种度量方法中，相关性、距离、信息增益、一致性属于筛选器，而分类器错误率属于封装器。

 >筛选器由于与具体的分类算法无关，因此其在不同的分类算法之间的推广能力较强，而且计算量也较小。而封装器由于在评价的过程中应用了具体的分类算法进行分类，因此其推广到其他分类算法的效果可能较差，而且计算量也较大。
 

特征选择能剔除不相关(irrelevant)或亢余(redundant )的特征，从而达到减少特征个数，提高模型精确度，减少运行时间的目的。另一方面，选取出真正相关的特征简化了模型，使研究人员易于理解数据产生的过程。

## 使用filter方法的特征选择

特征选择方法依据是否独立于后续的学习算法, 可分为过滤式 (Filter) 和封装式 (Wrapper)两种.
Filter 与后续学习算法无关, 一般直接利用所有训练数据的统计性能评估特征, 速度快, 但评估与后续学习算法的性能偏差较大. Wrapper 利用后续学习算法的训练准确率评估特征子集, 偏差小, 计算量大, 不适合大数据集. 
Filter 特征选择方法一般使用评价准则来增强特征与类的相关性, 削减特征之间的相关性. 可将评价函数分成 4 类: 距离度量、信息度量、依赖性度量以及一致性度量。
>其主要思想是：对每一维的特征“打分”，即给每一维的特征赋予权重，这样的权重就代表着该维特征的重要性，然后依据权重排序。

Wrapper 模型将特征选择算法作为学习算法的一个组成部分, 并且直接使用分类性能作为特征重要性程度的评价标准. 它的依据是选择子集最终被用于构造分类模型. 因此, 若在构造分类模型时, 直接采用那些能取得较高分类性能的特征即可, 从而获得一个分类性能较高的分类模型. 该方法在速度上要比 Filter 方法慢, 但是它所选择的优化特征子集的规模相对要小得多, 非常有利于关键特征的辨识; 同时它的准确率比较高, 但泛化能力比较差, 时间复杂度较高. 
>其主要思想是：将子集的选择看作是一个搜索寻优问题，生成不同的组合，对组合进行评价，再与其他的组合进行比较。这样就将子集的选择看作是一个是一个优化问题，这里有很多的优化算法可以解决，尤其是一些启发式的优化算法，如GA，PSO，DE，ABC等。

作者使用互信息方式，且提出一个新颖的距离度量方法
![](http://ww3.sinaimg.cn/large/4542e63ejw1f76nwwoaicj20ew0423z0.jpg)
图中dm为manhattan距离，distmax为最大manhattan距离。
![](http://ww2.sinaimg.cn/large/4542e63ejw1f76nzhormvj20cj01wwel.jpg)
公式表明，如果一个对象与其最近邻有较大的距离的话，说明其很有可能为局外点，其表示的信息有较少的价值。
作者提出的CRM（综合相关性度量）结合了MI、SE、mRW（modified relief weight），使用MI指导特征选择，使用香家熵来避免过拟合和欠拟合，relief weight本用于双分类问题中评估单独特征，在这里被扩展用在多分类问题及评估单一特征组合后的相关性。

## 目标识别
作者在集成学习方案中使用DST进行水雷分类，使用集成学习框架来集成单一分类器的分类结果，使用DST来决定单一分类器对最后的贡献度，其派生的BBA分为两个部分，一是分类器对假设提供的支持度（object part），二是量化分类器本身的可靠度（classifer part）。目标部分的通常因目标不同而不同，而分类器部分是相对固定的。

为了使用DST，单一分类器的分类结果被看作  a piece of evdience，本文的BBA分成两个部分，object part是非经验部分，给出单一分类对假设的支持度。分类器部分量化分类器给出的判定结果质量。分类器可以看作是目标部分的权重或者折扣量。

![](http://ww1.sinaimg.cn/large/4542e63ejw1f76phteq5pj20cl03mjrr.jpg)

作者使用了kNN、KNND（assisted by DST）、SVMG（gaussina kernal）、PNN（Probabilistic neural network）

***

>一.D-S证据理论引入 
诞生 
　　D-S证据理论的诞生：起源于20世纪60年代的哈佛大学数学家A.P. Dempster利用上、下限概率解决多值映射问题，1967年起连续发表一系列论文，标志着证据理论的正式诞生。　　 
形成 
  dempster的学生G.shafer对证据理论做了进一步发展，引入信任函数概念，形成了一套“证据”和“组合”来处理不确定性推理的数学方法 
  D-S理论是对贝叶斯推理方法推广，主要是利用概率论中贝叶斯条件概率来进行的，需要知道先验概率。而D-S证据理论不需要知道先验概率，能够很好地表示“不确定”，被广泛用来处理不确定数据。 
　　适用于：信息融合、专家系统、情报分析、法律案件分析、多属性决策分析 
　　 
二.D-S证据理论的基本概念 

>定义1 基本概率分配（BPA） 
　 设U为以识别框架，则函数m:2u→[0,1]满足下列条件： 
　 (1)m(ϕ)=0 
　 (2)∑A⊂Um(A)=1时 
　 称m(A)=0为A的基本赋值，m(A)=0表示对A的信任程度 
　 也称为mass函数。 
　 
> 定义2 信任函数 （Belief Function） 
　 Bel:2u→[0,1] 
　 Bel(A)=∑B⊂Am(B)=1　(∀A⊂U) 
　 表示A的全部子集的基本概率分配函数之和 
　 
> 定义3 似然函数（plausibility Function） 
　 pl(A)=1−Bel(A¯¯¯)=∑B⊂Um(B)−∑B⊂A¯m(B)=∑B⋂A≠ϕm(B) 
　 似然函数表示不否认A的信任度，是所有与A相交的子集的基本概率分配之和。 
　 
> 定义4 信任区间 
　[Bel(A)，pl(A)]表示命题A的信任区间，Bel(A)表示信任函数为下限，pl(A)表示似真函数为 　上限 
　举例：如(0.25,0.85),表示A为真有0.25的信任度，A为假有0.15的信任度，A不确定度为0.6 

>三.D-S证据理论的组合规则 

>ｍ个mass函数的Dempster合成规则 
　Dempster合成规则 
　其中K称为归一化因子，1−K即∑A1⋂...⋂An=ϕm1(A1)⋅m2(A2)⋅⋅⋅mn(An)反 　应了证据的冲突程度

>四.判决规则 

>设存在A1,A2⊂U,满足 
　m(A1)=max{m(Ai),Ai⊂U} 
　m(A2)=max{m(Ai),Ai⊂U且Ai≠A1} 
　若有： 
　m(A1)−m(A2)>ε1 
　m(Θ)<ε2 
　m(A1)>m(Θ) 
　则A1为判决结果，ε1，ε2为预先设定的门限，Θ为不确定集合 
***
